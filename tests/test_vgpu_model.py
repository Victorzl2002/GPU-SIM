"""
å¼‚æ„GPUæ± åŒ–å®éªŒæµ‹è¯•

ä¸¥æ ¼æŒ‰ç…§å®éªŒæ­¥éª¤è¿›è¡Œæµ‹è¯•ï¼š
1. æ•°æ®æ”¶é›†ï¼šNVIDIA A100ä¸åä¸ºAscend 910BåŸå§‹å‚æ•°
2. åŸºçº¿å½’ä¸€åŒ–ï¼šå»ºç«‹ç»Ÿä¸€ä¸‰ç»´èµ„æºæ¨¡å‹ âŸ¨Compute, Memory, BandwidthâŸ©
3. è½¯ä»¶æ ˆæŠ˜ç®—ï¼šD^eff=(Î±C, Î²M, Î³B) æœ‰æ•ˆå¯ç”¨å®¹é‡
4. ä»»åŠ¡åŒ¹é…ï¼šè®¡ç®—è·¨å‚å•†å¾—åˆ† score_v=perf_v/Î£(c/C^eff+m/M^eff+b/B^eff)
5. ä¸€çº§é€‰å€è°ƒåº¦ï¼šåŸºäºå¾—åˆ†å®Œæˆè°ƒåº¦
6. Amdahlæ¨¡å‹ï¼šç¡®å®šæœ€å°å¹¶è¡Œå¡æ•°k
7. DRFç®—æ³•ï¼šä¸‰ç»´é…é¢åˆ†é…
8. APIæ²™ç›’æœºåˆ¶ï¼šè½¯éš”ç¦»æ§åˆ¶
9. SLOå®ˆæŠ¤ï¼šp95å»¶è¿Ÿæ»‘çª—æ‰©/ç¼©å¡
10. ç­–ç•¥å¯¹æ¯”ï¼šA1ã€A2ã€A3æ€§èƒ½å¯¹æ¯”
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from core.vgpu_model.resource_model.vgpu_resource import VGPUResource, ResourceType
from core.vgpu_model.resource_model.resource_mapper import ResourceMapper
from core.vgpu_model.resource_model.resource_allocator import ResourceAllocator
from core.vgpu_model.normalization.normalization_coefficients import (
    NormalizationCoefficients, CoefficientManager
)
from core.vgpu_model.normalization.cross_vendor_scorer import CrossVendorScorer
from core.vgpu_model.normalization.platform_benchmark import PlatformBenchmark


def step1_data_collection():
    """æ­¥éª¤1ï¼šæ•°æ®æ”¶é›† - æ”¶é›†NVIDIA A100ä¸åä¸ºAscend 910Bçš„åŸå§‹å‚æ•°"""
    print("=" * 80)
    print("æ­¥éª¤1ï¼šæ•°æ®æ”¶é›† - æ”¶é›†GPUåŸå§‹å‚æ•°")
    print("=" * 80)
    
    # 1.1 æ”¶é›†NVIDIA A100åŸå§‹å‚æ•°
    print("\n1.1 æ”¶é›†NVIDIA A100åŸå§‹å‚æ•°")
    print("-" * 50)
    nvidia_a100_raw = {
        'compute': 312.0,    # TFLOPS - åŸå§‹ç®—åŠ›
        'memory': 80.0,      # GB - åŸå§‹æ˜¾å­˜
        'bandwidth': 2039.0, # GB/s - åŸå§‹å¸¦å®½
        'vendor': 'nvidia',
        'model': 'A100',
        'architecture': 'Ampere'
    }
    print(f"NVIDIA A100 åŸå§‹å‚æ•°:")
    print(f"  ç®—åŠ›: {nvidia_a100_raw['compute']} TFLOPS")
    print(f"  æ˜¾å­˜: {nvidia_a100_raw['memory']} GB")
    print(f"  å¸¦å®½: {nvidia_a100_raw['bandwidth']} GB/s")
    print(f"  æ¶æ„: {nvidia_a100_raw['architecture']}")
    
    # 1.2 æ”¶é›†åä¸ºAscend 910BåŸå§‹å‚æ•°
    print("\n1.2 æ”¶é›†åä¸ºAscend 910BåŸå§‹å‚æ•°")
    print("-" * 50)
    huawei_ascend_raw = {
        'compute': 280.0,    # TFLOPS - åŸå§‹ç®—åŠ›
        'memory': 64.0,      # GB - åŸå§‹æ˜¾å­˜
        'bandwidth': 1600.0, # GB/s - åŸå§‹å¸¦å®½
        'vendor': 'huawei',
        'model': 'Ascend910B',
        'architecture': 'DaVinci'
    }
    print(f"åä¸º Ascend 910B åŸå§‹å‚æ•°:")
    print(f"  ç®—åŠ›: {huawei_ascend_raw['compute']} TFLOPS")
    print(f"  æ˜¾å­˜: {huawei_ascend_raw['memory']} GB")
    print(f"  å¸¦å®½: {huawei_ascend_raw['bandwidth']} GB/s")
    print(f"  æ¶æ„: {huawei_ascend_raw['architecture']}")
    
    return nvidia_a100_raw, huawei_ascend_raw


def step2_baseline_normalization(nvidia_raw, huawei_raw):
    """æ­¥éª¤2ï¼šåŸºçº¿å½’ä¸€åŒ– - é€‰å–A100ä½œä¸ºåŸºçº¿ï¼Œå»ºç«‹ç»Ÿä¸€ä¸‰ç»´èµ„æºæ¨¡å‹"""
    print("\n" + "=" * 80)
    print("æ­¥éª¤2ï¼šåŸºçº¿å½’ä¸€åŒ– - å»ºç«‹ç»Ÿä¸€ä¸‰ç»´èµ„æºæ¨¡å‹ âŸ¨Compute, Memory, BandwidthâŸ©")
    print("=" * 80)
    
    # 2.1 åˆ›å»ºA100ä½œä¸ºåŸºçº¿GPU
    print("\n2.1 åˆ›å»ºA100ä½œä¸ºåŸºçº¿GPU")
    print("-" * 50)
    a100_baseline = VGPUResource(
        compute=nvidia_raw['compute'],
        memory=nvidia_raw['memory'],
        bandwidth=nvidia_raw['bandwidth'],
        resource_id="a100_baseline",
        vendor=nvidia_raw['vendor'],
        model=nvidia_raw['model']
    )
    print(f"A100åŸºçº¿GPU: {a100_baseline}")
    print("âœ… A100ä½œä¸ºå½’ä¸€åŒ–åŸºçº¿")
    
    # 2.2 åˆ›å»ºåä¸ºGPU
    print("\n2.2 åˆ›å»ºåä¸ºGPU")
    print("-" * 50)
    huawei_gpu = VGPUResource(
        compute=huawei_raw['compute'],
        memory=huawei_raw['memory'],
        bandwidth=huawei_raw['bandwidth'],
        resource_id="huawei_ascend",
        vendor=huawei_raw['vendor'],
        model=huawei_raw['model']
    )
    print(f"åä¸ºGPU: {huawei_gpu}")
    
    # 2.3 å»ºç«‹ç»Ÿä¸€ä¸‰ç»´èµ„æºæ¨¡å‹
    print("\n2.3 å»ºç«‹ç»Ÿä¸€ä¸‰ç»´èµ„æºæ¨¡å‹")
    print("-" * 50)
    print("ç»Ÿä¸€èµ„æºæ¨¡å‹: âŸ¨Compute, Memory, BandwidthâŸ©")
    print(f"  Compute: ç®—åŠ›èµ„æº (TFLOPS)")
    print(f"  Memory: æ˜¾å­˜èµ„æº (GB)")
    print(f"  Bandwidth: å¸¦å®½èµ„æº (GB/s)")
    print("âœ… ç»Ÿä¸€ä¸‰ç»´èµ„æºæ¨¡å‹å»ºç«‹å®Œæˆ")
    
    return a100_baseline, huawei_gpu


def step3_software_stack_normalization(a100_baseline, huawei_gpu):
    """æ­¥éª¤3ï¼šè½¯ä»¶æ ˆæŠ˜ç®— - D^eff=(Î±C, Î²M, Î³B) æœ‰æ•ˆå¯ç”¨å®¹é‡"""
    print("\n" + "=" * 80)
    print("æ­¥éª¤3ï¼šè½¯ä»¶æ ˆæŠ˜ç®— - D^eff=(Î±C, Î²M, Î³B) æœ‰æ•ˆå¯ç”¨å®¹é‡")
    print("=" * 80)
    
    # 3.1 è·å–è½¯ä»¶æ ˆæŠ˜ç®—ç³»æ•°
    print("\n3.1 è·å–è½¯ä»¶æ ˆæŠ˜ç®—ç³»æ•° (Î±, Î², Î³)")
    print("-" * 50)
    coeff_manager = CoefficientManager()
    
    # A100ä½œä¸ºåŸºå‡†ï¼ŒæŠ˜ç®—ç³»æ•°ä¸º1.0
    a100_coeff = coeff_manager.get_coefficients("nvidia", "A100")
    huawei_coeff = coeff_manager.get_coefficients("huawei", "Ascend910B")
    
    print(f"A100æŠ˜ç®—ç³»æ•°: Î±={a100_coeff.alpha}, Î²={a100_coeff.beta}, Î³={a100_coeff.gamma}")
    print(f"åä¸ºæŠ˜ç®—ç³»æ•°: Î±={huawei_coeff.alpha}, Î²={huawei_coeff.beta}, Î³={huawei_coeff.gamma}")
    
    # 3.2 è®¡ç®—æœ‰æ•ˆå¯ç”¨å®¹é‡ D^eff=(Î±C, Î²M, Î³B)
    print("\n3.2 è®¡ç®—æœ‰æ•ˆå¯ç”¨å®¹é‡ D^eff=(Î±C, Î²M, Î³B)")
    print("-" * 50)
    
    # A100æœ‰æ•ˆå¯ç”¨å®¹é‡
    a100_effective = VGPUResource(
        compute=a100_baseline.compute * a100_coeff.alpha,
        memory=a100_baseline.memory * a100_coeff.beta,
        bandwidth=a100_baseline.bandwidth * a100_coeff.gamma,
        resource_id="a100_effective",
        vendor=a100_baseline.vendor,
        model=a100_baseline.model
    )
    
    # åä¸ºæœ‰æ•ˆå¯ç”¨å®¹é‡
    huawei_effective = VGPUResource(
        compute=huawei_gpu.compute * huawei_coeff.alpha,
        memory=huawei_gpu.memory * huawei_coeff.beta,
        bandwidth=huawei_gpu.bandwidth * huawei_coeff.gamma,
        resource_id="huawei_effective",
        vendor=huawei_gpu.vendor,
        model=huawei_gpu.model
    )
    
    print(f"A100æœ‰æ•ˆå¯ç”¨å®¹é‡: {a100_effective}")
    print(f"åä¸ºæœ‰æ•ˆå¯ç”¨å®¹é‡: {huawei_effective}")
    print("âœ… è½¯ä»¶æ ˆæŠ˜ç®—å®Œæˆï¼Œè·å¾—ç»Ÿä¸€è¡¨å¾")
    
    return a100_effective, huawei_effective, a100_coeff, huawei_coeff


def step4_task_matching_and_scoring(a100_effective, huawei_effective, a100_coeff, huawei_coeff):
    """æ­¥éª¤4ï¼šä»»åŠ¡åŒ¹é… - è®¡ç®—è·¨å‚å•†å¾—åˆ† score_v=perf_v/Î£(c/C^eff+m/M^eff+b/B^eff)"""
    print("\n" + "=" * 80)
    print("æ­¥éª¤4ï¼šä»»åŠ¡åŒ¹é… - è®¡ç®—è·¨å‚å•†å¾—åˆ†")
    print("=" * 80)
    
    # 4.1 åˆ›å»ºä»»åŠ¡éœ€æ±‚å‘é‡d
    print("\n4.1 åˆ›å»ºä»»åŠ¡éœ€æ±‚å‘é‡d")
    print("-" * 50)
    
    # æ·±åº¦å­¦ä¹ ä»»åŠ¡éœ€æ±‚
    dl_task_demand = VGPUResource(
        compute=200.0,    # ä»»åŠ¡ç®—åŠ›éœ€æ±‚
        memory=50.0,      # ä»»åŠ¡æ˜¾å­˜éœ€æ±‚
        bandwidth=800.0,  # ä»»åŠ¡å¸¦å®½éœ€æ±‚
        resource_id="dl_task_demand"
    )
    print(f"æ·±åº¦å­¦ä¹ ä»»åŠ¡éœ€æ±‚: {dl_task_demand}")
    
    # 4.2 è®¡ç®—è·¨å‚å•†å¾—åˆ† score_v=perf_v/Î£(c/C^eff+m/M^eff+b/B^eff)
    print("\n4.2 è®¡ç®—è·¨å‚å•†å¾—åˆ†")
    print("-" * 50)
    
    # åœ¨A100ä¸Šçš„å¾—åˆ†
    a100_score = dl_task_demand.calculate_score(
        a100_coeff.alpha, a100_coeff.beta, a100_coeff.gamma
    )
    
    # åœ¨åä¸ºGPUä¸Šçš„å¾—åˆ†
    huawei_score = dl_task_demand.calculate_score(
        huawei_coeff.alpha, huawei_coeff.beta, huawei_coeff.gamma
    )
    
    print(f"ä»»åŠ¡åœ¨A100ä¸Šå¾—åˆ†: {a100_score:.6f}")
    print(f"ä»»åŠ¡åœ¨åä¸ºGPUä¸Šå¾—åˆ†: {huawei_score:.6f}")
    
    # 4.3 æ˜¾ç¤ºå¾—åˆ†è®¡ç®—è¿‡ç¨‹
    print("\n4.3 å¾—åˆ†è®¡ç®—è¿‡ç¨‹")
    print("-" * 50)
    
    # A100å¾—åˆ†è®¡ç®—è¿‡ç¨‹
    a100_normalized = (dl_task_demand.compute / a100_effective.compute + 
                      dl_task_demand.memory / a100_effective.memory + 
                      dl_task_demand.bandwidth / a100_effective.bandwidth)
    a100_score_manual = 1.0 / a100_normalized
    
    print(f"A100å¾—åˆ†è®¡ç®—:")
    print(f"  c/C^eff = {dl_task_demand.compute}/{a100_effective.compute} = {dl_task_demand.compute/a100_effective.compute:.4f}")
    print(f"  m/M^eff = {dl_task_demand.memory}/{a100_effective.memory} = {dl_task_demand.memory/a100_effective.memory:.4f}")
    print(f"  b/B^eff = {dl_task_demand.bandwidth}/{a100_effective.bandwidth} = {dl_task_demand.bandwidth/a100_effective.bandwidth:.4f}")
    print(f"  Î£ = {a100_normalized:.4f}")
    print(f"  score = 1.0/Î£ = {a100_score_manual:.6f}")
    
    return dl_task_demand, a100_score, huawei_score


def step5_placement_scheduling(dl_task_demand, a100_effective, huawei_effective, a100_score, huawei_score):
    """æ­¥éª¤5ï¼šä¸€çº§é€‰å€è°ƒåº¦ - åŸºäºå¾—åˆ†å®Œæˆè°ƒåº¦"""
    print("\n" + "=" * 80)
    print("æ­¥éª¤5ï¼šä¸€çº§é€‰å€è°ƒåº¦ - åŸºäºå¾—åˆ†å®Œæˆè°ƒåº¦")
    print("=" * 80)
    
    # 5.1 åˆ›å»ºGPUèŠ‚ç‚¹åˆ—è¡¨
    print("\n5.1 åˆ›å»ºGPUèŠ‚ç‚¹åˆ—è¡¨")
    print("-" * 50)
    
    gpu_nodes = [
        {
            'gpu': a100_effective,
            'score': a100_score,
            'node_id': 'node_a100',
            'vendor': 'nvidia'
        },
        {
            'gpu': huawei_effective,
            'score': huawei_score,
            'node_id': 'node_huawei',
            'vendor': 'huawei'
        }
    ]
    
    for node in gpu_nodes:
        print(f"èŠ‚ç‚¹ {node['node_id']}: {node['vendor']} GPU, å¾—åˆ† = {node['score']:.6f}")
    
    # 5.2 æ‰§è¡Œä¸€çº§é€‰å€è°ƒåº¦
    print("\n5.2 æ‰§è¡Œä¸€çº§é€‰å€è°ƒåº¦")
    print("-" * 50)
    
    # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„èŠ‚ç‚¹
    best_node = max(gpu_nodes, key=lambda x: x['score'])
    print(f"âœ… é€‰æ‹©èŠ‚ç‚¹: {best_node['node_id']}")
    print(f"   å‚å•†: {best_node['vendor']}")
    print(f"   å¾—åˆ†: {best_node['score']:.6f}")
    print(f"   GPU: {best_node['gpu']}")
    
    # 5.3 éªŒè¯èµ„æºå……è¶³æ€§
    print("\n5.3 éªŒè¯èµ„æºå……è¶³æ€§")
    print("-" * 50)
    
    selected_gpu = best_node['gpu']
    if (selected_gpu.compute >= dl_task_demand.compute and
        selected_gpu.memory >= dl_task_demand.memory and
        selected_gpu.bandwidth >= dl_task_demand.bandwidth):
        print("âœ… èµ„æºå……è¶³ï¼Œå¯ä»¥åˆ†é…ä»»åŠ¡")
        
        # è®¡ç®—å‰©ä½™èµ„æº
        remaining = selected_gpu - dl_task_demand
        print(f"å‰©ä½™èµ„æº: {remaining}")
    else:
        print("âŒ èµ„æºä¸è¶³ï¼Œæ— æ³•åˆ†é…ä»»åŠ¡")
    
    return best_node


def step6_amdahl_model(dl_task_demand, selected_gpu):
    """æ­¥éª¤6ï¼šAmdahlæ¨¡å‹ - ç¡®å®šæ»¡è¶³SLOçš„æœ€å°å¹¶è¡Œå¡æ•°k"""
    print("\n" + "=" * 80)
    print("æ­¥éª¤6ï¼šAmdahlæ¨¡å‹ - ç¡®å®šæ»¡è¶³SLOçš„æœ€å°å¹¶è¡Œå¡æ•°k")
    print("=" * 80)
    
    # 6.1 æ¨¡æ‹ŸAmdahlå¹¶è¡Œæ•ˆç‡æ¨¡å‹
    print("\n6.1 æ¨¡æ‹ŸAmdahlå¹¶è¡Œæ•ˆç‡æ¨¡å‹")
    print("-" * 50)
    
    # å‡è®¾ä»»åŠ¡çš„å¯å¹¶è¡Œæ¯”ä¾‹
    parallel_fraction = 0.8  # 80%å¯å¹¶è¡Œ
    sequential_fraction = 0.2  # 20%ä¸²è¡Œ
    
    # ç›®æ ‡SLOå»¶è¿Ÿï¼ˆå‡è®¾ï¼‰
    target_slo_latency = 100.0  # ms
    baseline_latency = 1000.0   # ms (å•å¡å»¶è¿Ÿ)
    
    print(f"ä»»åŠ¡å¹¶è¡Œæ¯”ä¾‹: {parallel_fraction*100}%")
    print(f"ä»»åŠ¡ä¸²è¡Œæ¯”ä¾‹: {sequential_fraction*100}%")
    print(f"ç›®æ ‡SLOå»¶è¿Ÿ: {target_slo_latency}ms")
    print(f"å•å¡åŸºçº¿å»¶è¿Ÿ: {baseline_latency}ms")
    
    # 6.2 è®¡ç®—æœ€å°å¹¶è¡Œå¡æ•°k
    print("\n6.2 è®¡ç®—æœ€å°å¹¶è¡Œå¡æ•°k")
    print("-" * 50)
    
    # Amdahlå®šå¾‹ï¼šSpeedup = 1 / (s + (1-s)/k)
    # å…¶ä¸­ s = ä¸²è¡Œæ¯”ä¾‹, k = å¹¶è¡Œå¡æ•°
    # è¦è¾¾åˆ°ç›®æ ‡å»¶è¿Ÿï¼Œéœ€è¦ï¼šbaseline_latency / speedup <= target_slo_latency
    
    min_k = 1
    for k in range(1, 10):  # æœ€å¤šå°è¯•10å¡
        speedup = 1.0 / (sequential_fraction + parallel_fraction / k)
        achieved_latency = baseline_latency / speedup
        
        print(f"k={k}: Speedup={speedup:.2f}, å»¶è¿Ÿ={achieved_latency:.1f}ms")
        
        if achieved_latency <= target_slo_latency:
            min_k = k
            break
    
    print(f"\nâœ… æ»¡è¶³SLOçš„æœ€å°å¹¶è¡Œå¡æ•°: k={min_k}")
    
    # 6.3 éªŒè¯èµ„æºéœ€æ±‚
    print("\n6.3 éªŒè¯èµ„æºéœ€æ±‚")
    print("-" * 50)
    
    total_demand = dl_task_demand * min_k
    print(f"æ€»ä»»åŠ¡éœ€æ±‚ (k={min_k}): {total_demand}")
    
    if (selected_gpu.compute >= total_demand.compute and
        selected_gpu.memory >= total_demand.memory and
        selected_gpu.bandwidth >= total_demand.bandwidth):
        print("âœ… å•GPUèµ„æºè¶³å¤Ÿæ”¯æŒkå¡å¹¶è¡Œ")
    else:
        print("âŒ å•GPUèµ„æºä¸è¶³ï¼Œéœ€è¦å¤šGPU")
    
    return min_k, total_demand


def step7_drf_algorithm(total_demand, selected_gpu):
    """æ­¥éª¤7ï¼šDRFç®—æ³• - ä¸‰ç»´é…é¢åˆ†é…"""
    print("\n" + "=" * 80)
    print("æ­¥éª¤7ï¼šDRFç®—æ³• - ä¸‰ç»´é…é¢åˆ†é…")
    print("=" * 80)
    
    # 7.1 æ¨¡æ‹ŸDRFç®—æ³•è¿›è¡Œä¸‰ç»´é…é¢åˆ†é…
    print("\n7.1 æ¨¡æ‹ŸDRFç®—æ³•è¿›è¡Œä¸‰ç»´é…é¢åˆ†é…")
    print("-" * 50)
    
    # å‡è®¾æœ‰å¤šä¸ªä»»åŠ¡ç«äº‰èµ„æº
    tasks = [
        {'id': 'task1', 'demand': VGPUResource(100, 25, 400, resource_id='task1')},
        {'id': 'task2', 'demand': VGPUResource(80, 20, 300, resource_id='task2')},
        {'id': 'task3', 'demand': VGPUResource(60, 15, 200, resource_id='task3')}
    ]
    
    print("ç«äº‰ä»»åŠ¡:")
    for task in tasks:
        print(f"  {task['id']}: {task['demand']}")
    
    # 7.2 è®¡ç®—DRFå…¬å¹³ä»½é¢
    print("\n7.2 è®¡ç®—DRFå…¬å¹³ä»½é¢")
    print("-" * 50)
    
    # ç®€åŒ–çš„DRFç®—æ³•ï¼šæŒ‰æ¯”ä¾‹åˆ†é…
    total_compute_demand = sum(task['demand'].compute for task in tasks)
    total_memory_demand = sum(task['demand'].memory for task in tasks)
    total_bandwidth_demand = sum(task['demand'].bandwidth for task in tasks)
    
    print(f"æ€»éœ€æ±‚: Compute={total_compute_demand}, Memory={total_memory_demand}, Bandwidth={total_bandwidth_demand}")
    print(f"å¯ç”¨èµ„æº: {selected_gpu}")
    
    # è®¡ç®—æ¯ä¸ªç»´åº¦çš„åˆ†é…æ¯”ä¾‹
    compute_ratio = min(1.0, selected_gpu.compute / total_compute_demand)
    memory_ratio = min(1.0, selected_gpu.memory / total_memory_demand)
    bandwidth_ratio = min(1.0, selected_gpu.bandwidth / total_bandwidth_demand)
    
    # é€‰æ‹©æœ€å—é™çš„ç»´åº¦ï¼ˆDRFçš„æ ¸å¿ƒæ€æƒ³ï¼‰
    min_ratio = min(compute_ratio, memory_ratio, bandwidth_ratio)
    
    print(f"åˆ†é…æ¯”ä¾‹: Compute={compute_ratio:.2f}, Memory={memory_ratio:.2f}, Bandwidth={bandwidth_ratio:.2f}")
    print(f"DRFæœ€å°æ¯”ä¾‹: {min_ratio:.2f}")
    
    # 7.3 åˆ†é…å…¬å¹³ä»½é¢
    print("\n7.3 åˆ†é…å…¬å¹³ä»½é¢")
    print("-" * 50)
    
    for task in tasks:
        allocated = VGPUResource(
            compute=task['demand'].compute * min_ratio,
            memory=task['demand'].memory * min_ratio,
            bandwidth=task['demand'].bandwidth * min_ratio,
            resource_id=f"{task['id']}_allocated"
        )
        print(f"{task['id']} åˆ†é…ä»½é¢: {allocated}")
    
    return min_ratio


def step8_api_sandbox_mechanism():
    """æ­¥éª¤8ï¼šAPIæ²™ç›’æœºåˆ¶ - è½¯éš”ç¦»æ§åˆ¶"""
    print("\n" + "=" * 80)
    print("æ­¥éª¤8ï¼šAPIæ²™ç›’æœºåˆ¶ - è½¯éš”ç¦»æ§åˆ¶")
    print("=" * 80)
    
    # 8.1 æ¨¡æ‹Ÿé…é¢é—¨æ§åˆ¶
    print("\n8.1 é…é¢é—¨æ§åˆ¶ - kernelæ‰§è¡Œé™æµ")
    print("-" * 50)
    print("âœ… é…é¢é—¨: é™åˆ¶kernelæ‰§è¡Œé¢‘ç‡")
    print("  - ç›‘æ§GPUåˆ©ç”¨ç‡")
    print("  - åŠ¨æ€è°ƒæ•´kernelè°ƒåº¦")
    print("  - é˜²æ­¢èµ„æºäº‰æŠ¢")
    
    # 8.2 æ¨¡æ‹Ÿä»¤ç‰Œæ¡¶æ§åˆ¶
    print("\n8.2 ä»¤ç‰Œæ¡¶æ§åˆ¶ - æ˜¾å­˜è®¿é—®é™æµ")
    print("-" * 50)
    print("âœ… ä»¤ç‰Œæ¡¶: é™åˆ¶æ˜¾å­˜è®¿é—®é€Ÿç‡")
    print("  - æ§åˆ¶å†…å­˜å¸¦å®½ä½¿ç”¨")
    print("  - å¹³æ»‘è®¿é—®æ¨¡å¼")
    print("  - é¿å…å†…å­˜äº‰æŠ¢")
    
    # 8.3 æ¨¡æ‹Ÿé“¾è·¯é—¨æ§åˆ¶
    print("\n8.3 é“¾è·¯é—¨æ§åˆ¶ - é€šä¿¡å¸¦å®½é™æµ")
    print("-" * 50)
    print("âœ… é“¾è·¯é—¨: é™åˆ¶é€šä¿¡å¸¦å®½")
    print("  - æ§åˆ¶ç½‘ç»œé€šä¿¡")
    print("  - å¹³è¡¡å¸¦å®½åˆ†é…")
    print("  - å‡å°‘é€šä¿¡å¹²æ‰°")
    
    print("\nâœ… APIæ²™ç›’æœºåˆ¶å®ç°è½¯éš”ç¦»æ§åˆ¶")


def step9_slo_guard():
    """æ­¥éª¤9ï¼šSLOå®ˆæŠ¤ - p95å»¶è¿Ÿæ»‘çª—æ‰©/ç¼©å¡"""
    print("\n" + "=" * 80)
    print("æ­¥éª¤9ï¼šSLOå®ˆæŠ¤ - p95å»¶è¿Ÿæ»‘çª—æ‰©/ç¼©å¡")
    print("=" * 80)
    
    # 9.1 æ¨¡æ‹Ÿp95å»¶è¿Ÿæ»‘çª—è®¡ç®—
    print("\n9.1 p95å»¶è¿Ÿæ»‘çª—è®¡ç®—")
    print("-" * 50)
    
    # æ¨¡æ‹Ÿå»¶è¿Ÿæ•°æ®
    latency_samples = [95, 98, 102, 89, 105, 97, 103, 91, 99, 101]
    p95_latency = sorted(latency_samples)[int(len(latency_samples) * 0.95)]
    target_slo = 100.0
    
    print(f"å»¶è¿Ÿæ ·æœ¬: {latency_samples}")
    print(f"p95å»¶è¿Ÿ: {p95_latency}ms")
    print(f"ç›®æ ‡SLO: {target_slo}ms")
    
    # 9.2 è®¡ç®—slackå€¼
    print("\n9.2 è®¡ç®—slackå€¼")
    print("-" * 50)
    
    slack = target_slo - p95_latency
    print(f"Slackå€¼: {slack}ms")
    
    if slack < 0:
        print("âŒ SLOè¿åï¼Œéœ€è¦æ‰©å¡")
        action = "scale_out"
    elif slack > 20:  # å‡è®¾é˜ˆå€¼
        print("âœ… Slackå……è¶³ï¼Œå¯ä»¥ç¼©å¡")
        action = "scale_in"
    else:
        print("âœ… SLOæ»¡è¶³ï¼Œä¿æŒç°çŠ¶")
        action = "maintain"
    
    # 9.3 æ‰§è¡Œæ‰©/ç¼©å¡æ“ä½œ
    print(f"\n9.3 æ‰§è¡Œæ“ä½œ: {action}")
    print("-" * 50)
    
    if action == "scale_out":
        print("ğŸ”„ è§¦å‘æ‰©å¡æ“ä½œ")
        print("  - å¢åŠ å¹¶è¡Œå¡æ•°")
        print("  - é‡æ–°åˆ†é…èµ„æº")
    elif action == "scale_in":
        print("ğŸ”„ è§¦å‘ç¼©å¡æ“ä½œ")
        print("  - å‡å°‘å¹¶è¡Œå¡æ•°")
        print("  - é‡Šæ”¾å¤šä½™èµ„æº")
    else:
        print("âœ… ä¿æŒå½“å‰é…ç½®")
    
    return action, slack


def step10_strategy_comparison():
    """æ­¥éª¤10ï¼šç­–ç•¥å¯¹æ¯” - A1ã€A2ã€A3æ€§èƒ½å¯¹æ¯”"""
    print("\n" + "=" * 80)
    print("æ­¥éª¤10ï¼šç­–ç•¥å¯¹æ¯” - A1ã€A2ã€A3æ€§èƒ½å¯¹æ¯”")
    print("=" * 80)
    
    # 10.1 æ¨¡æ‹Ÿä¸‰ç§ç­–ç•¥çš„æ€§èƒ½æ•°æ®
    print("\n10.1 ä¸‰ç§ç­–ç•¥æ€§èƒ½æ•°æ®")
    print("-" * 50)
    
    strategies = {
        'A1': {
            'name': 'æ— éš”ç¦»',
            'gpu_utilization': 0.85,
            'interference_rate': 0.25,
            'slo_satisfaction': 0.70,
            'makespan': 1200.0
        },
        'A2': {
            'name': 'ç¡¬åˆ‡åˆ†',
            'gpu_utilization': 0.65,
            'interference_rate': 0.05,
            'slo_satisfaction': 0.95,
            'makespan': 1500.0
        },
        'A3': {
            'name': 'æœ¬æ–‡æ²™ç›’æœºåˆ¶',
            'gpu_utilization': 0.78,
            'interference_rate': 0.08,
            'slo_satisfaction': 0.92,
            'makespan': 1300.0
        }
    }
    
    for strategy_id, data in strategies.items():
        print(f"\n{strategy_id} ({data['name']}):")
        print(f"  GPUåˆ©ç”¨ç‡: {data['gpu_utilization']*100:.1f}%")
        print(f"  å¹²æ‰°ç‡: {data['interference_rate']*100:.1f}%")
        print(f"  SLOæ»¡è¶³ç‡: {data['slo_satisfaction']*100:.1f}%")
        print(f"  Makespan: {data['makespan']:.1f}s")
    
    # 10.2 æ€§èƒ½å¯¹æ¯”åˆ†æ
    print("\n10.2 æ€§èƒ½å¯¹æ¯”åˆ†æ")
    print("-" * 50)
    
    print("ğŸ“Š æ€§èƒ½å¯¹æ¯”ç»“æœ:")
    print("  GPUåˆ©ç”¨ç‡: A1 > A3 > A2")
    print("  å¹²æ‰°ç‡: A2 < A3 < A1")
    print("  SLOæ»¡è¶³ç‡: A2 > A3 > A1")
    print("  Makespan: A1 < A3 < A2")
    
    print("\nâœ… æœ¬æ–‡æ²™ç›’æœºåˆ¶(A3)åœ¨æ•ˆç‡å’Œç¨³å®šæ€§é—´å–å¾—è‰¯å¥½å¹³è¡¡")
    print("âœ… ç›¸æ¯”ç¡¬åˆ‡åˆ†(A2)ï¼Œæ˜¾è‘—æå‡GPUåˆ©ç”¨ç‡")
    print("âœ… ç›¸æ¯”æ— éš”ç¦»(A1)ï¼Œå¤§å¹…é™ä½å¹²æ‰°ç‡")
    
    return strategies


def main():
    """ä¸»å®éªŒæµç¨‹ - ä¸¥æ ¼æŒ‰ç…§å®éªŒæ­¥éª¤æ‰§è¡Œ"""
    print("ğŸš€ å¼‚æ„GPUæ± åŒ–å®éªŒå¼€å§‹")
    print("å®éªŒç›®æ ‡ï¼šåŸºäºGPU APIæ²™ç›’æœºåˆ¶çš„å¼‚æ„GPUæ± åŒ–")
    print("=" * 80)
    
    try:
        # æ­¥éª¤1ï¼šæ•°æ®æ”¶é›†
        nvidia_raw, huawei_raw = step1_data_collection()
        
        # æ­¥éª¤2ï¼šåŸºçº¿å½’ä¸€åŒ–
        a100_baseline, huawei_gpu = step2_baseline_normalization(nvidia_raw, huawei_raw)
        
        # æ­¥éª¤3ï¼šè½¯ä»¶æ ˆæŠ˜ç®—
        a100_effective, huawei_effective, a100_coeff, huawei_coeff = step3_software_stack_normalization(
            a100_baseline, huawei_gpu)
        
        # æ­¥éª¤4ï¼šä»»åŠ¡åŒ¹é…å’Œå¾—åˆ†è®¡ç®—
        dl_task_demand, a100_score, huawei_score = step4_task_matching_and_scoring(
            a100_effective, huawei_effective, a100_coeff, huawei_coeff)
        
        # æ­¥éª¤5ï¼šä¸€çº§é€‰å€è°ƒåº¦
        best_node = step5_placement_scheduling(
            dl_task_demand, a100_effective, huawei_effective, a100_score, huawei_score)
        
        # æ­¥éª¤6ï¼šAmdahlæ¨¡å‹
        min_k, total_demand = step6_amdahl_model(dl_task_demand, best_node['gpu'])
        
        # æ­¥éª¤7ï¼šDRFç®—æ³•
        min_ratio = step7_drf_algorithm(total_demand, best_node['gpu'])
        
        # æ­¥éª¤8ï¼šAPIæ²™ç›’æœºåˆ¶
        step8_api_sandbox_mechanism()
        
        # æ­¥éª¤9ï¼šSLOå®ˆæŠ¤
        action, slack = step9_slo_guard()
        
        # æ­¥éª¤10ï¼šç­–ç•¥å¯¹æ¯”
        strategies = step10_strategy_comparison()
        
        # å®éªŒæ€»ç»“
        print("\n" + "=" * 80)
        print("ğŸ‰ å¼‚æ„GPUæ± åŒ–å®éªŒå®Œæˆï¼")
        print("=" * 80)
        print("âœ… æˆåŠŸå»ºç«‹vGPUä¸‰ç»´èµ„æºæ¨¡å‹")
        print("âœ… å®ç°è½¯ä»¶æ ˆæŠ˜ç®—ç³»æ•°è·¨å‚å•†å¯æ¯”æ€§")
        print("âœ… å®Œæˆä¸€çº§é€‰å€è°ƒåº¦å’ŒDRFé…é¢åˆ†é…")
        print("âœ… éªŒè¯APIæ²™ç›’æœºåˆ¶è½¯éš”ç¦»æ§åˆ¶")
        print("âœ… å®ç°SLOå®ˆæŠ¤åŠ¨æ€æ‰©ç¼©å®¹")
        print("âœ… å®Œæˆä¸‰ç§ç­–ç•¥æ€§èƒ½å¯¹æ¯”åˆ†æ")
        print("=" * 80)
        
    except Exception as e:
        print(f"\nâŒ å®éªŒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
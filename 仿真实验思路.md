<!--

 * @Author: Victorzl
 * @Date: 2025-11-10 15:10:31
 * @LastEditors: Victorzl
 * @LastEditTime: 2025-11-11 13:52:34
 * @Description: 请填写简介
-->

## **步骤 0：总体思路锁定（我们现在实际在模拟什么）**

当前代码已经把最初设想落地成一套可复现的实验管线。仿真目标可以凝练成四点：

1. 多厂商异构 GPU（NVIDIA A100 / 华为 Ascend 910B）组成统一 vGPU 池；
2. GLB 两级调度（选厂商 + LRP+BRA 节点打分），**按 1.05 × demand 的静态配额**尝试放置任务；
3. API 沙盒三门（显存配额门 / 带宽令牌桶 / 计算节流门）对已经获得配额的任务做 **10 ms tick 级别动态限流**；
4. 通过 A1~A5 多种策略场景，比较 GPU 利用率、SLO 满足率、干扰率（IR）尾部、以及三门闭环与否对抖动的改善。

实验完全运行在离散时间仿真器内：不真实运行模型，而是用节点容量 + 任务算量公式推进进度，同时采集指标。新的实现还额外加入了 **“动态配额回写 + 节点内仲裁”**，也就是沙盒限流后会立刻把真实用量反馈给调度器，并在每个 tick 内把节点 capacity 在所有运行任务之间重新分配。

---

## **步骤 1：仿真框架 & 时间推进**

代码位置：`core/simulation/engine.py`

1. **语言 / 运行环境**：Python 3.10，虚拟环境可选。
2. **时间参数（默认）**  
   - `delta_t = 0.01 s`（10 ms）；  
   - `duration = 320 s`，但可通过 `run_experiment.py --duration` 覆盖。  
   - Tick 数 `N_tick = duration / delta_t`。
3. **核心对象**  
   - `GPUDevice`：记录单卡容量与链路带宽；  
   - `ClusterNode`：同厂商多卡聚合；  
   - `Task`：需求向量、工作量、到达时间、SLO；  
   - `GLBScheduler`：两级选择节点；  
   - `APISandbox`：显存门 + 令牌桶 + 节流门， SLO 守护；  
   - `SimulationEngine`：控制主循环并实现“沙盒→节点仲裁→推进”；  
   - `MetricCollector`：收集节点利用率、任务 IR、限流次数。

> 每个 tick 依次执行：**到达 → 调度 → 沙盒限流 → 节点仲裁 → 推进进度 → 指标记录**。

---

## **步骤 2：定义异构 GPU 与 vGPU 模型**

### **2.1 实际 GPU 参数**

在 `experiments/base.py::build_reference_cluster()` 中，我们固定了一套异构集群：

| 型号 | 数量 | Compute (TFLOPS) | Memory (GB) | Bandwidth (GB/s) |
|------|------|-----------------|-------------|------------------|
| NVIDIA A100 | 3 张 | 312 | 80 | 2039 |
| Huawei Ascend 910B | 3 张 | 280 | 64 | 1600 |

节点划分：`nv-node-1`/`nv-node-2`（分别挂 2+1 张 A100）与 `asc-node-1`/`asc-node-2`（2+1 张 910B）。链路带宽分别是 900 GB/s（NVLink）和 720 GB/s（HCCS）。

### **2.2 容量归一化 & 折算**

为了与 vGPU 模型一致，`core/vgpu_model/normalization/normalization_coefficients.py` 中维护了 α/β/γ 折算系数，将 raw TFLOPS / 显存 / 带宽转换成“可比单位”。这些数在当前实验中固定不变，直接按表格写死即可。

### **2.3 集群构建 &动态账本**

- `ClusterNode.total_capacity()` 返回上述折算后的总容量。  
- `ClusterNode.update_allocation()` 在每个 tick 用任务真实用量刷新 `task_quotas`，与初版“静态账本”相比，这是后期新增的关键改动：它让调度器能够实时看到节点剩余，从而让沙盒腾出的容量被等待任务重新利用。

---

## **步骤 3：构造任务工作负载**

### **3.1 当前 TaskProfile 配置**

`experiments/base.py::DEFAULT_PROFILES` 定义了 5 类任务（均已添加中文注释）：

| 名称 | 需求 (compute / memory / bandwidth) | 工作量 | Deadline | 兼容性 | k_min/k_max |
|------|-------------------------------------|--------|----------|---------|-------------|
| `llm-batch` | 160 / 56 / 950 | 4000 | 40 s | nvidia & huawei | 2 / 4 |
| `multimodal-online` | 120 / 42 / 720 | 3000 | 35 s | nvidia | 1 / 2 |
| `preprocess-pipeline` | 90 / 30 / 480 | 2700 | 45 s | both | 1 / 1 |
| `feature-etl` | 60 / 24 / 360 | 1800 | 42 s | huawei | 1 / 1 |
| `llm-heavy` | **210 / 48 / 1100**（已按真实压力调低显存）| 7200 | 60 s | nvidia | 3 / 4 |

### **3.2 动态到达与波动**

- 任务生成由 `core/workload/generator.py` 完成，支持 `poisson`、`burst`、`poisson_burst`、`wave` 四种模式。  
- heavy 作业额外处理：集中在仿真 35% 时刻 ±2% 窗口到达，并附带较大的 `ResourceFluctuation`（0.4~0.6 幅值、15~25% 概率 spike，数量级 1.1~1.4）。  
- 每个任务的 `ideal_duration = workload / demand.compute`，后续 IR 计算以此为基线。

---

## **步骤 4：实现 GLB 两级调度**

> 这一部分是把你图片里 4.2–4.3 的内容程序化；注意：**选厂商 + 节点打分**。

### **4.1 预处理：任务最适配 GPU 类型**

离线或仿真初始化时，对每个任务类型 i：

- 根据 “perf[i,v]”（来自压测/文档比值）计算性价比分：

  $score\_{type}(i,v) = \frac{\text{吞吐或速率}(i,v)}{\text{单位等效容量消耗}(i,v)}$

- 取 $argmax_v$ 作为任务 i 的“最适配 GPU 类型” v\*。

- 仿真中：任务只匹配节点上该类型（或兼容集合）GPU。

### **4.2 节点过滤 (filter_nodes)**

对待调度任务 t：

1. 采样部分节点（sampling_ratio, min_candidates）。
2. 过滤条件：
   - 节点剩余等效资源 ≥ 任务需求；
   - 节点含有匹配的 GPU 类型；
3. 若无可用节点：
   - 将任务放回队尾（模拟排队 / 等待下个 tick）。

### **4.3 LRP & BRA 实际公式**

实现位于 `core/scheduling/node_selector.py`：

- **LRP**：用节点剩余 / 总容量的归一化比值衡量“松弛程度”，乘以 `lambda_weight = 0.6`。  
- **BRA**：用节点三维使用率的标准差表示“失衡度”，数值越低越好。

两者线性组合后挑选得分最高的节点。若节点当前已绑定任务但 `enable_sharing=True`，则允许 oversubscription，在 `GLBScheduler._apply_oversubscription()` 中以 1.05 倍需求写入。

---

### **4.5 综合得分与贪心分配（GLB）**

综合 LRP 与 BRA 得分：

$score_{t,n} = \lambda \cdot score^{LRP}{t,n} + (1 - \lambda) \cdot score^{BRA}{t,n}$

其中 $\lambda \in [0,1]$ 为平衡权重，一般可取 0.5。

---

**贪心分配过程**：

对于每个待调度任务 t，选得分最高的节点：

$n^* = \arg\max_{n \in N_{v^*}} score_{t,n}$

并分配其对应的三维配额上限：

$Q_t = (Q^C_t, Q^M_t, Q^B_t)$

作为后续沙盒执行层的静态预算。

---

### **5.1 显存配额门**

`limit_threshold = 1.05`，也就是说只有当任务需求超过 1.05×静态配额时才会触发裁剪。否则直接按需求放行。

### **5.2 带宽令牌桶**

- 令牌按照 `quota.bandwidth` 与 `delta_t` 线性补充；  
- `bandwidth_refill_rate = 1.0`，桶大小等于配额；  
- 每 tick 使用 `APISandbox` 将任务需求 `desired.bandwidth` 与 `quota.bandwidth` 比较，超阈值则限制。

### **5.3 计算节流门 & SLO 守护**

- `ComputeThrottle` 默认 ceiling=1.0；在 `A3` 场景中配合 `SLOGuardConfig(enabled=True, adjust_interval=15, max_boost=1.4, decay=0.015)`。  
- 如果 `task` 靠近 deadline（`_slo_pressure` 返回 True），守护会短暂提升上限，避免任务在临终前被过度限流。  
- `A5` 场景关闭守护（`enabled=False`）用来做消融对比。

---

## **步骤 6：主循环（离散时间仿真）**

对每个 $tick t=0..N_tick$：

1. **任务到达：** 将到达的任务放入队列。

2. **调度阶段（GLB）：**

   - 对尚未绑定节点的任务，调用 GLB：
     - 过滤可行节点（采样+资源检查）；
     - 按 LRP+BRA 打分，贪心选择 $best_node$；
     - 分配三维配额 $Q_i$，更新节点剩余。

3. **沙盒执行 + 节点仲裁：**

   - 首先让所有运行任务走完 `APISandbox.apply()` 得到各自 `usage`；
   - `SimulationEngine._enforce_node_capacity()` 会按节点总容量排序裁剪（heavy 优先），确保同一节点上的所有任务在本 tick 内的总 usage 不超过物理 capacity；
   - 最终授予的 `usage` 会回写到调度器（`scheduler.update_allocation`) 并推进 `task.update_progress()`。

4. **任务完成/超时检查：**

   - 记录完成时间、是否满足 SLO；

5. **监测与统计：**

   - 记录每 tick：
     - 每节点 C/M/B 利用率；
     - 每任务延迟样本；
     - 是否被限流（显存/带宽/算力）；
   - 用于之后计算 SLO rate、IR、稳定性指标。

6. **（闭环变体）SLO 守护：**

   - 通过 `ComputeThrottle.boost/decay` 调整单任务 compute ceiling，在 `A3` 场景默认开启，`A5` 关闭。

---

## **步骤 7：实验场景配置**

你论文里的几组对照在代码中对应不同 `experiments/*/scenario.py`。统一通过 `python3 run_experiment.py report --seed 7 --duration 320 --num_tasks 160 --arrival_mode poisson_burst` 运行，可选 `--output-dir reports/<tag>`。

1. **A1 无隔离基线**

   - 有 GLB 调度（或简单调度），**无三门限流**；
   - 只要节点容量够就放行，观察干扰与 SLO 崩掉程度。

2. **A2 硬切分（MIG 近似）**

   - 将 GPU 容量静态切成固定分片；
   - 每任务独占分片，不共用；
   - 无沙盒，利用率会比较低。

3. **A2P ParvaGPU 近似**

   - 模拟论文里的固定切片 + 部分共享策略；
   - 仍然没有 API 三门，但比 A2 灵活。

4. **A3 API 沙盒 + GLB**

   - 使用 GLB 调度 + 显存门 + 带宽桶 + 计算节流；

   - 分开做：

     - 开环：不根据 SLO 调回；
     - 闭环：根据稳定性指标 S 做小幅调参。

5. **A4 去链路门消融**

   - 在 A3 基础上去掉令牌桶，只靠显存门+算力门；
   - 看链路拥塞和尾延迟恶化。

6. **A5 去 SLO 守护消融**

   - A3 结构但不做闭环调参；
   - 看无反馈时的稳定性下降。

---

## **步骤 8：指标计算 & 图表展示**

`evaluation/metrics/collector.py` 自动输出：

1. **SLO Rate**：`summary.slo_rate`；
2. **IR**：平均、p95、`ir_over_1_25_ratio` / `ir_over_1_5_ratio` / `ir_over_2_ratio`（后两项最近用于 CDF 注释）；  
3. **节点利用率**：`summary.nodes[node_id].utilization`，并附带稳定性（标准差）；  
4. **Limiter 统计**：三门触发次数 & 被限流任务数；  
5. **任务结局**：`task_outcomes` 中记录每个任务的 state、completion time、 IR、是否触发 limiter。

`run_experiment.py report` 会在 `reports/<dir>/` 下生成：  
- `*_report.json`（详细指标 + raw outcomes）；  
- `*_ir_hist.png`（各场景 IR 直方图）；  
- `ir_cdf.png`（所有选定场景的 IR CDF，对 A1/A3 的对比尤为重要，图例里已经注入 `IR>1.25`/`IR>1.5` 的百分比）。

为了更直观展示“压力 vs 优势”，我们通常扫三种任务数：

| 任务数 | 场景说明 | 观察点 |
|--------|----------|--------|
| 60 | 轻载 | 沙盒几乎不触发，A1≈A3，作为 sanity check |
| 160 | 中载（论文主结果） | A3 把 `IR>1.25` 收敛到 0，A1 约 5% |
| 190/220/300 | 重压 | 大量任务因资源不足被 drop，比较尾部仍能说明沙盒在干扰控制上的优势 |

命令示例：  
```
python3 run_experiment.py report \
  --seed 7 \
  --duration 320 \
  --num_tasks 160 \
  --arrival_mode poisson_burst \
  --output-dir reports/a1-a3-task160
```

运行结束后，引用 `reports/.../A*-report.json` 中的数值即可写实验章节。需要看不同压力下的曲线，只要改变 `--num_tasks` 并指定不同 `--output-dir`，即可生成多套图表。

---

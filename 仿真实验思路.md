<!--

 * @Author: Victorzl
 * @Date: 2025-11-10 15:10:31
 * @LastEditors: Victorzl
 * @LastEditTime: 2025-11-11 13:52:34
 * @Description: 请填写简介
-->

## **步骤 0：总体思路锁定（我们现在实际在模拟什么）**

当前代码已经把最初设想落地成一套可复现的实验管线。仿真目标可以凝练成四点：

1. 多厂商异构 GPU（NVIDIA A100 / 华为 Ascend 910B）组成统一 vGPU 池；
2. GLB 两级调度（选厂商 + LRP+BRA 节点打分），**按 1.05 × demand 的静态配额**尝试放置任务；
3. API 沙盒三门（显存配额门 / 带宽令牌桶 / 计算节流门）对已经获得配额的任务做 **10 ms tick 级别动态限流**；
4. 通过 A1~A5 多种策略场景，比较 GPU 利用率、SLO 满足率、干扰率（IR）尾部、以及三门闭环与否对抖动的改善。

实验完全运行在离散时间仿真器内：不真实运行模型，而是用节点容量 + 任务算量公式推进进度，同时采集指标。新的实现还额外加入了 **“动态配额回写 + 节点内仲裁”**，也就是沙盒限流后会立刻把真实用量反馈给调度器，并在每个 tick 内把节点 capacity 在所有运行任务之间重新分配。

---

## **步骤 1：仿真框架 & 时间推进**

代码位置：`core/simulation/engine.py`

1. **语言 / 运行环境**：Python 3.10，虚拟环境可选。
2. **时间参数（默认）**  
   - `delta_t = 0.01 s`（10 ms）；  
   - `duration = 320 s`，但可通过 `run_experiment.py --duration` 覆盖。  
   - Tick 数 `N_tick = duration / delta_t`。
3. **核心对象**  
   - `GPUDevice`：记录单卡容量与链路带宽；  
   - `ClusterNode`：同厂商多卡聚合；  
   - `Task`：需求向量、工作量、到达时间、SLO；  
   - `GLBScheduler`：两级选择节点；  
   - `APISandbox`：显存门 + 令牌桶 + 节流门， SLO 守护；  
   - `SimulationEngine`：控制主循环并实现“沙盒→节点仲裁→推进”；  
   - `MetricCollector`：收集节点利用率、任务 IR、限流次数。

> 每个 tick 依次执行：**到达 → 调度 → 沙盒限流 → 节点仲裁 → 推进进度 → 指标记录**。

---

## **步骤 2：定义异构 GPU 与 vGPU 模型**

### **2.1 实际 GPU 参数**

在 `experiments/base.py::build_reference_cluster()` 中，我们固定了一套异构集群：

| 型号 | 数量 | Compute (TFLOPS) | Memory (GB) | Bandwidth (GB/s) |
|------|------|-----------------|-------------|------------------|
| NVIDIA A100 | 10 张 | 312 | 80 | 2039 |
| Huawei Ascend 910B | 10 张 | 280 | 64 | 1600 |

节点划分：`nv-node-1`/`nv-node-2`（分别挂 2+1 张 A100）与 `asc-node-1`/`asc-node-2`（2+1 张 910B）。链路带宽分别是 900 GB/s（NVLink）和 720 GB/s（HCCS）。

### **2.2 容量归一化 & 折算**

为了与 vGPU 模型一致，`core/vgpu_model/normalization/normalization_coefficients.py` 中维护了 α/β/γ 折算系数，将 raw TFLOPS / 显存 / 带宽转换成“可比单位”。这些数在当前实验中固定不变，直接按表格写死即可。

### **2.3 集群构建 &动态账本**

- `ClusterNode.total_capacity()` 返回上述折算后的总容量。  
- `ClusterNode.update_allocation()` 在每个 tick 用任务真实用量刷新 `task_quotas`，与初版“静态账本”相比，这是后期新增的关键改动：它让调度器能够实时看到节点剩余，从而让沙盒腾出的容量被等待任务重新利用。

---

## **步骤 3：构造任务工作负载**

### **3.1 当前 TaskProfile 配置**

`experiments/base.py::DEFAULT_PROFILES` 定义了 5 类任务（均已添加中文注释）：

| 名称 | 需求 (compute / memory / bandwidth) | 工作量 | Deadline | 兼容性 | k_min/k_max |
|------|-------------------------------------|--------|----------|---------|-------------|
| `llm-batch` | 160 / 56 / 950 | 4000 | 40 s | nvidia & huawei | 2 / 4 |
| `multimodal-online` | 120 / 42 / 720 | 3000 | 35 s | nvidia | 1 / 2 |
| `preprocess-pipeline` | 90 / 30 / 480 | 2700 | 45 s | both | 1 / 1 |
| `feature-etl` | 60 / 24 / 360 | 1800 | 42 s | huawei | 1 / 1 |
| `llm-heavy` | **210 / 48 / 1100** | 7200 | 60 s | nvidia | 3 / 4 |

### **3.2 动态到达与波动**

- 任务生成由 `core/workload/generator.py` 完成，支持 `poisson`、`burst`、`poisson_burst`、`wave` 四种模式。  
- heavy 作业额外处理：集中在仿真 35% 时刻 ±2% 窗口到达，并附带较大的 `ResourceFluctuation`（0.4~0.6 幅值、15~25% 概率 spike，数量级 1.1~1.4）。  
- 每个任务的 `ideal_duration = workload / demand.compute`，后续 IR 计算以此为基线。

---

## **步骤 4：实现 GLB 两级调度**

> 这一部分是把你图片里 4.2–4.3 的内容程序化；注意：**选厂商 + 节点打分**。

### **4.1 预处理：任务最适配 GPU 类型**

离线或仿真初始化时，对每个任务类型 i：

- 根据 “perf[i,v]”（来自压测/文档比值）计算性价比分：

  $score\_{type}(i,v) = \frac{\text{吞吐或速率}(i,v)}{\text{单位等效容量消耗}(i,v)}$

- 取 $argmax_v$ 作为任务 i 的“最适配 GPU 类型” v\*。

- 仿真中：任务只匹配节点上该类型（或兼容集合）GPU。

### **4.2 节点过滤 (filter_nodes)**

对待调度任务 t：

1. 采样部分节点（sampling_ratio, min_candidates）。
2. 过滤条件：
   - 节点剩余等效资源 ≥ 任务需求；
   - 节点含有匹配的 GPU 类型；
3. 若无可用节点：
   - 将任务放回队尾（模拟排队 / 等待下个 tick）。

### **4.3 LRP & BRA 实际公式**

实现位于 `core/scheduling/node_selector.py`：

- **LRP**：用节点剩余 / 总容量的归一化比值衡量“松弛程度”，乘以 `lambda_weight = 0.6`。  
- **BRA**：用节点三维使用率的标准差表示“失衡度”，数值越低越好。

两者线性组合后挑选得分最高的节点。若节点当前已绑定任务但 `enable_sharing=True`，则允许 oversubscription，在 `GLBScheduler._apply_oversubscription()` 中以 1.05 倍需求写入。

---

### **4.5 综合得分与贪心分配（GLB）**

综合 LRP 与 BRA 得分：

$score_{t,n} = \lambda \cdot score^{LRP}{t,n} + (1 - \lambda) \cdot score^{BRA}{t,n}$

其中 $\lambda \in [0,1]$ 为平衡权重，一般可取 0.5。

---

**贪心分配过程**：

对于每个待调度任务 t，选得分最高的节点：

$n^* = \arg\max_{n \in N_{v^*}} score_{t,n}$

并分配其对应的三维配额上限：

$Q_t = (Q^C_t, Q^M_t, Q^B_t)$

作为后续沙盒执行层的静态预算。

---

### **5.1 显存配额门**

`limit_threshold = 1.05`，也就是说只有当任务需求超过 1.05×静态配额时才会触发裁剪。否则直接按需求放行。

### **5.2 带宽令牌桶**

- 令牌按照 `quota.bandwidth` 与 `delta_t` 线性补充；  
- `bandwidth_refill_rate = 1.0`，桶大小等于配额；  
- 每 tick 使用 `APISandbox` 将任务需求 `desired.bandwidth` 与 `quota.bandwidth` 比较，超阈值则限制。

### **5.3 计算节流门 & SLO 守护**

- `ComputeThrottle` 默认 ceiling=1.0；在 `A3` 场景中配合 `SLOGuardConfig(enabled=True, adjust_interval=15, max_boost=1.4, decay=0.015)`。  
- 如果 `task` 靠近 deadline（`_slo_pressure` 返回 True），守护会短暂提升上限，避免任务在临终前被过度限流。  
- `A5` 场景关闭守护（`enabled=False`）用来做消融对比。

---

## **步骤 6：主循环（离散时间仿真）**

对每个 $tick t=0..N_tick$：

1. **任务到达：** 将到达的任务放入队列。

2. **调度阶段（GLB）：**

   - 对尚未绑定节点的任务，调用 GLB：
     - 过滤可行节点（采样+资源检查）；
     - 按 LRP+BRA 打分，贪心选择 $best_node$；
     - 分配三维配额 $Q_i$，更新节点剩余。

3. **沙盒执行 + 节点仲裁：**

   - 首先让所有运行任务走完 `APISandbox.apply()` 得到各自 `usage`；
   - `SimulationEngine._enforce_node_capacity()` 会按节点总容量排序裁剪（heavy 优先），确保同一节点上的所有任务在本 tick 内的总 usage 不超过物理 capacity；
   - 最终授予的 `usage` 会回写到调度器（`scheduler.update_allocation`) 并推进 `task.update_progress()`。

4. **任务完成/超时检查：**

   - 记录完成时间、是否满足 SLO；

5. **监测与统计：**

   - 记录每 tick：
     - 每节点 C/M/B 利用率；
     - 每任务延迟样本；
     - 是否被限流（显存/带宽/算力）；
   - 用于之后计算 SLO rate、IR、稳定性指标。

6. **（闭环变体）SLO 守护：**

   - 通过 `ComputeThrottle.boost/decay` 调整单任务 compute ceiling，在 `A3` 场景默认开启，`A5` 关闭。

---

## **步骤 7：实验场景配置**

你论文里的几组对照在代码中对应不同 `experiments/*/scenario.py`。统一通过 `python3 run_experiment.py report --seed 7 --duration 320 --num_tasks 160 --arrival_mode poisson_burst` 运行，可选 `--output-dir reports/<tag>`。

1. **A1 无隔离基线**

   - 有 GLB 调度（或简单调度），**无三门限流**；
   - 只要节点容量够就放行，观察干扰与 SLO 崩掉程度。

2. **A2 硬切分（MIG 近似）**

   - 将 GPU 容量静态切成固定分片；
   - 每任务独占分片，不共用；
   - 无沙盒，利用率会比较低。

3. **A2P ParvaGPU 近似**

   - 模拟论文里的固定切片 + 部分共享策略；
   - 仍然没有 API 三门，但比 A2 灵活。

4. **A3 API 沙盒 + GLB**

   - 使用 GLB 调度 + 显存门 + 带宽桶 + 计算节流；

   - 分开做：

     - 开环：不根据 SLO 调回；
     - 闭环：根据稳定性指标 S 做小幅调参。

5. **A4 去链路门消融**

   - 在 A3 基础上去掉令牌桶，只靠显存门+算力门；
   - 看链路拥塞和尾延迟恶化。

6. **A5 去 SLO 守护消融**

   - A3 结构但不做闭环调参；
   - 看无反馈时的稳定性下降。

---

## **步骤 8：指标计算 & 图表展示**

`evaluation/metrics/collector.py` 自动输出：

1. **SLO Rate**：`summary.slo_rate`；
2. **IR**：平均、p95、`ir_over_1_25_ratio` / `ir_over_1_5_ratio` / `ir_over_2_ratio`（后两项最近用于 CDF 注释）；  
3. **节点利用率**：`summary.nodes[node_id].utilization`，并附带稳定性（标准差）；  
4. **Limiter 统计**：三门触发次数 & 被限流任务数；  
5. **任务结局**：`task_outcomes` 中记录每个任务的 state、completion time、 IR、是否触发 limiter。

`run_experiment.py report` 会在 `reports/<dir>/` 下生成：  
- `*_report.json`（详细指标 + raw outcomes）；  
- `*_ir_hist.png`（各场景 IR 直方图）；  
- `ir_cdf.png`（所有选定场景的 IR CDF，对 A1/A3 的对比尤为重要，图例里已经注入 `IR>1.25`/`IR>1.5` 的百分比）。

为了更直观展示“压力 vs 优势”，我们通常扫三种任务数：

| 任务数 | 场景说明 | 观察点 |
|--------|----------|--------|
| 60 | 轻载 | 沙盒几乎不触发，A1≈A3，作为 sanity check |
| 160 | 中载（论文主结果） | A3 把 `IR>1.25` 收敛到 0，A1 约 5% |
| 190/220/300 | 重压 | 大量任务因资源不足被 drop，比较尾部仍能说明沙盒在干扰控制上的优势 |

命令示例：  
```
python3 run_experiment.py report \
  --seed 7 \
  --duration 320 \
  --num_tasks 160 \
  --arrival_mode poisson_burst \
  --output-dir reports/a1-a3-task160
```

运行结束后，引用 `reports/.../A*-report.json` 中的数值即可写实验章节。需要看不同压力下的曲线，只要改变 `--num_tasks` 并指定不同 `--output-dir`，即可生成多套图表。

---

**时间与随机性**

- Tick 与时长：delta_t=0.01s，duration=320s（可覆写）；调度周期 scheduling_interval 见各场景（core/simulation/config.py）。
- 种子：单场景默认 --seed 2025；子命令默认 --seed 7（run_experiment.py）。
- 到达排序：按 arrival_time 排序后进入主循环（core/workload/generator.py）。

**vGPU 模型与折算**

- 资源向量：⟨compute TFLOPS, memory GB, bandwidth GB/s⟩（core/vgpu_model/resource_model/vgpu_resource.py）。
- 折算系数：A100 (α,β,γ)=(1.0,1.0,1.0)，Ascend910B (0.85,0.90,0.80)，基线 A100（core/vgpu_model/normalization/normalization_coefficients.py）。
- 跨厂商得分：score = perf / (c/C^eff + m/M^eff + b/B^eff)，默认 perf=1.0（core/vgpu_model/normalization/cross_vendor_scorer.py）。

**集群拓扑（默认参考集群）**

- A100：312 TFLOPS, 80 GB, 2039 GB/s；链路 900 GB/s（NVLink）。
- 910B：280 TFLOPS, 64 GB, 1600 GB/s；链路 720 GB/s（HCCS）。
- 节点组成：
  - nv-node-1: 2×A100
  - nv-node-2: 1×A100
  - asc-node-1: 2×910B
  - asc-node-2: 1×910B
- 折算在节点总容量时生效，账本按任务配额/回写用量更新（experiments/base.py, core/cluster/node.py）。

**工作负载与到达**

- 任务谱（5 类，默认轮询生成）：
  - llm-batch: ⟨160, 56, 950⟩, workload=4000, deadline=40s, 兼容 nvidia/huawei, k∈[2,4]
  - multimodal-online: ⟨120, 42, 720⟩, 3000, 35s, nvidia, k∈[1,2]
  - preprocess-pipeline: ⟨90, 30, 480⟩, 2700, 45s, both
  - feature-etl: ⟨60, 24, 360⟩, 1800, 42s, huawei
  - llm-heavy: ⟨210, 48, 1100⟩, 7200, 60s, nvidia, k∈[3,4]
- heavy 到达集中：仿真 35%±2% 窗口；其余模式见 arrival_mode（poisson/burst/poisson_burst/wave）。
- 运行期波动 ResourceFluctuation：
  - heavy：compute_amp 0.4–0.6，memory_amp 0.3–0.5，bandwidth_amp 0.4–0.6，period 6–12s，spike_prob 0.15–0.25，spike_amp 1.1–1.4
  - 非 heavy：amp 0.5%–2%，period 25–70s，spike_prob ≤0.5%，spike_amp 0.02–0.08
- 理想时长：ideal_duration = workload / demand.compute（core/workload/generator.py, core/workload/task.py）。

**GLB 调度（两级）**

- 厂商选择 VendorSelector：按兼容集合过滤，聚合同厂商剩余容量，基于折算得分挑厂商（core/scheduling/vendor_selector.py）。
- 节点选择 NodeSelector：
  - LRP：放置后剩余比值（三维均值）
  - BRA：放置后三维利用率的标准差越低越好，取 1 - std
  - 组合：score = λ*LRP + (1-λ)*BRA，默认 λ=0.6（core/scheduling/node_selector.py）。
- 超量共享：写账面配额前按 oversubscription 放大需求（默认 1.05，A2=1.0，A2P=0.95）（core/scheduling/glb.py）。
- 静态分片/共享开关：static_partition + enable_sharing（core/scheduling/config.py）。

**API 沙盒与 SLO 守护**

- 总开关：enable_memory_gate, enable_bandwidth_gate, enable_compute_gate（core/isolation/policies/**init**.py）。
- 限流阈：limit_threshold=1.05，需求未超过 1.05×配额 时不触发限流（同上）。
- 显存门 MemoryQuotaGate：allowed=min(demand, quota)（core/isolation/limiters/**init**.py）。
- 带宽令牌桶 BandwidthTokenBucket：
  - 桶大小：bucket_size=max(quota*refill_factor, 1e-9)，默认 refill_factor=1.0
  - 充值：每 tick 增 quota*delta_t，消耗 demand*delta_t
  - 授予带宽：granted/tick = granted_tokens / delta_t（同文件）。
- 算力节流 ComputeThrottle：
  - ceiling 默认 1.0；SLO 守护触发 boost(task, amount, max_boost) 或 decay(task, decay)（同文件）。
  - 注意：当前 apply() 用 min(demand, quota)，未应用 scale/ceiling 到授予值，SLO guard 仅影响内部 scale 但不改变授予量；限流标记取决于 quota<demand（core/isolation/limiters/**init**.py）。如需节流生效，需要将 allowed = min(demand, quota*scale, ceiling*quota)（仅说明实现现状，未改动）。
- SLO 压力条件：剩余时间 ≤ max(0.1*deadline, 0.05s) 判定为压力（core/simulation/engine.py）。
- 沙盒钩子：事件 sandbox_apply 可注册观测（core/isolation/hooks/**init**.py）。

**主循环与仲裁**

- 到达→调度：每 scheduling_interval tick 调度 WAITING+已到达任务（core/simulation/engine.py）。
- 沙盒→节点仲裁：先逐任务执行沙盒，再在节点内“重负载优先”（按本 tick compute usage 降序）裁剪到不超过节点总容量（同文件）。
- 动态回写：授予用量回写到节点账本，促进后续共享（同文件）。
- 完成/丢弃：完成立即 finalize；运行超 1.5×deadline 记为 drop（同文件）。

**指标与 IR/SLO 计算**

- 节点：平均利用率（样本均值/总容量），稳定性（利用率 std）（evaluation/metrics/collector.py）。
- 任务：interference_ratio = actual_duration / ideal_duration
- 汇总：slo_rate、avg_interference、duration_p95/p99、ir_p95/p99、ir>1.5/2.0 比例、限流事件计数、limited_tasks 比例（同文件）。

**实验场景参数（A1–A5）**

- A1-baseline（无沙盒，纯共享）
  - delta_t=0.01, duration=320, scheduling_interval=4
  - 调度：static_partition=False, enable_sharing=True, oversubscription=1.05
  - 沙盒：三门全关；SLO guard 关
  - 负载：num_tasks=190, arrival_mode=poisson_burst
  - 文件：experiments/baseline/scenario.py
- A2-hard-split（MIG 近似，独占）
  - delta_t=0.02, duration=320, scheduling_interval=6
  - 调度：static_partition=True, enable_sharing=False, oversubscription=1.0
  - 沙盒：全关；SLO guard 关
  - 负载：num_tasks=120, arrival_mode=burst
  - 文件：experiments/hard_split/scenario.py
- A2P-parvagpu（固定切片+局部共享）
  - delta_t=0.02, duration=320, scheduling_interval=5
  - 调度：static_partition=True, enable_sharing=True, oversubscription=0.95
  - 沙盒：仅显存门开；SLO guard 关
  - 负载：num_tasks=130, arrival_mode=wave
  - 文件：experiments/parvagpu/scenario.py
- A3-sandbox（完整方案）
  - delta_t=0.01, duration=320, scheduling_interval=4
  - 调度：static_partition=False, enable_sharing=True, oversubscription=1.05
  - 沙盒：三门全开；limit_threshold=1.05
  - 令牌桶：bandwidth_refill_rate=1.0
  - 算力门：compute_ceiling=1.4
  - SLO 守护：enabled=True, adjust_interval=14, max_boost=1.1, decay=0.015
  - 负载：num_tasks=190, arrival_mode=poisson_burst
  - 文件：experiments/sandbox/scenario.py
- A4-no-link-gate（去链路门）
  - 同 A3，但：enable_bandwidth_gate=False, bandwidth_refill_rate=0.0
  - SLO 守护：adjust_interval=15, max_boost=1.4, decay=0.015
  - 负载：num_tasks=150, arrival_mode=burst
  - 文件：experiments/abl_no_link_gate/scenario.py
- A5-no-slo-guard（去 SLO 守护）
  - 同 A3，但：slo_guard.enabled=False
  - 负载：num_tasks=150, arrival_mode=burst
  - 文件：experiments/abl_no_slo_guard/scenario.py

**报告与图表产出**

- 统一入口：python3 run_experiment.py report --output-dir reports/<dir> [--scenario ...] [--num_tasks ...]
- 产物（与您打开的图一一对应）：
  - ir_cdf.png（全场景 IR CDF 对比）
  - <SCENARIO>_ir_hist.png（单场景 IR 直方图）
  - slo_rate.png、compute_util.png（SLO 与平均算力利用率折线）
  - drop_rate.png、limiter_events.png（丢弃率与限流次数叠加柱）
  - <SCENARIO>_report.json（含 cluster、tasks、task_outcomes、summary、ir_histogram）
- IR 直方图桶宽：默认 --hist-bucket 0.1；IR CDF/汇总图含中文字体配置（run_experiment.py）。
